{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UtIxDSlFkAz",
        "outputId": "862b1f1f-b8d8-4f0a-b4c0-1d88fa14adf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed pygame-2.1.0\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m909.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Requirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.10/dist-packages (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[classic_control]\n",
        "!pip install keras-rl2\n",
        "!pip install protobuf==3.20.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras as k\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras import __version__\n",
        "tf.keras.__version__ = __version__\n",
        "\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, MultiBinary\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "45zLfwjI0iOa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOHZ-lWVFkA1"
      },
      "source": [
        "# 1. Test Random Environment with OpenAI Gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tit_for_tat(round,opp_move):\n",
        "  if round == 200:\n",
        "    return 1\n",
        "  else:\n",
        "    return opp_move"
      ],
      "metadata": {
        "id": "slqrNaZxkpyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8618ed71-bdfe-43c2-bcf4-9624dd54fa8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cooperate():\n",
        "  return 1\n",
        "\n",
        "def defect():\n",
        "  return 0\n",
        "\n",
        "def rand():\n",
        "  return random.randint(0,1)"
      ],
      "metadata": {
        "id": "0_k-aGaSlk88"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tit_for_two_tat(round, opp_move1, opp_move2):\n",
        "  if round >= 199 or opp_move1 == 1 or opp_move2 == 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "qJ-_QaSWmrHS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grudger(defected):\n",
        "  if defected:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "2OsXZr5PoCQY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pavlov(round, last_move,opp_move):\n",
        "  if round == 200:\n",
        "    return 1\n",
        "  elif last_move == opp_move or (last_move == 0 and opp_move == 1):\n",
        "    return last_move\n",
        "  elif last_move == 1 and opp_move == 0:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "aKwESrgpo-_l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_grudger(last_defect):\n",
        "  if last_defect < 4:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "Y0fPJBILqAb0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fortress(round, opp_move1, opp_move2):\n",
        "  if round == 200 or opp_move1 == 0 or opp_move2 == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "UvA-jcztroSv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alternator(round,last_move):\n",
        "  if round == 200 or last_move == 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Z25hJ-ptsWSc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sneaky(round,opp_move):\n",
        "  dice=random.randint(0,9)\n",
        "  if round == 200:\n",
        "    return 1\n",
        "  elif dice == 0:\n",
        "    return 0;\n",
        "  else:\n",
        "    return opp_move"
      ],
      "metadata": {
        "id": "cWeLDrvetS04"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom(round):\n",
        "  if round%4 == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "uF5DyEIEYxCu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# last 10 moves environment"
      ],
      "metadata": {
        "id": "9M9haSEcIjdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrisonEnv(Env):\n",
        "    def __init__(self):\n",
        "        self.action_space = Discrete(2)\n",
        "        self.observation_space = MultiBinary(10)\n",
        "        self.round = 200\n",
        "        self.state = np.array([-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
        "        self.opp_strategy = random.randint(0,10)\n",
        "        self.opp_move1=1\n",
        "        self.opp_move2=1\n",
        "        self.last_move1=self.state[0]\n",
        "        self.last_move2=self.state[1]\n",
        "        self.opp_defected=False\n",
        "        self.user_defected=False\n",
        "        self.last_defect= np.Infinity\n",
        "        self.opp_last_defect= np.Infinity\n",
        "        self.move=1\n",
        "        self.opp_move=1\n",
        "\n",
        "        self.opp_score=0\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.opp_move2=self.opp_move1\n",
        "        self.opp_move1=self.opp_move\n",
        "        self.last_move2=self.state[1]\n",
        "        self.last_move1=self.state[0]\n",
        "\n",
        "        self.move=action\n",
        "\n",
        "\n",
        "\n",
        "        if self.opp_strategy == 0:\n",
        "          opp_move=tit_for_tat(self.round,self.last_move1)\n",
        "        elif self.opp_strategy == 1:\n",
        "          opp_move=cooperate()\n",
        "        elif self.opp_strategy == 2:\n",
        "          opp_move=sneaky(self.round, self.last_move1)\n",
        "        elif self.opp_strategy == 3:\n",
        "          opp_move=rand()\n",
        "        elif self.opp_strategy == 4:\n",
        "          opp_move=tit_for_two_tat(self.round,self.last_move1, self.last_move2)\n",
        "        elif self.opp_strategy == 5:\n",
        "          opp_move=grudger(self.user_defected)\n",
        "        elif self.opp_strategy == 6:\n",
        "          opp_move=pavlov(self.round,self.opp_move1, self.last_move1)\n",
        "        elif self.opp_strategy == 7:\n",
        "          opp_move=soft_grudger(self.last_defect)\n",
        "        elif self.opp_strategy == 8:\n",
        "          opp_move=fortress(self.round, self.last_move1, self.last_move2)\n",
        "        elif self.opp_strategy == 9:\n",
        "          opp_move=alternator(self.round,self.opp_move1)\n",
        "        elif self.opp_strategy == 10:\n",
        "          opp_move = custom(self.round)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        temp = np.array(list(self.state))\n",
        "\n",
        "        for i in range(1,10):\n",
        "          self.state[i] = temp[i-1]\n",
        "        self.state[0] = self.move\n",
        "\n",
        "\n",
        "        if opp_move == 0:\n",
        "          self.opp_defected=True\n",
        "          self.opp_last_defect=0\n",
        "        else:\n",
        "          self.opp_last_defect += 1\n",
        "\n",
        "        if self.move == 0:\n",
        "          self.user_defected=True\n",
        "          self.last_defect=0\n",
        "        else:\n",
        "          self.last_defect += 1\n",
        "\n",
        "        if self.move == 1 and opp_move == 1:\n",
        "          reward = 3\n",
        "          self.opp_score += 3\n",
        "        elif self.move == 1 and opp_move == 0:\n",
        "          reward = 0\n",
        "          self.opp_score += 5\n",
        "        elif self.move == 0 and opp_move == 1:\n",
        "          reward = 5\n",
        "          self.opp_score += 0\n",
        "        elif self.move == 0 and opp_move == 0:\n",
        "          reward = 1\n",
        "          self.opp_score += 1\n",
        "\n",
        "        self.round -= 1\n",
        "\n",
        "        if self.round <= 0:\n",
        "            done = True\n",
        "\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        info = {'opp_score':self.opp_score}\n",
        "\n",
        "\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.round = 200\n",
        "        self.state = np.array([-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
        "        self.opp_strategy = random.randint(0,10)\n",
        "        self.opp_move1=1\n",
        "        self.opp_move2=1\n",
        "        self.last_move1=self.state[0]\n",
        "        self.last_move2=self.state[1]\n",
        "        self.opp_defected=False\n",
        "        self.user_defected=False\n",
        "        self.last_defect= np.Infinity\n",
        "        self.opp_last_defect= np.Infinity\n",
        "        self.move=1\n",
        "        self.opp_move=1\n",
        "\n",
        "        self.opp_score = 0\n",
        "\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "iijDpCg0IySw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = PrisonEnv()\n",
        "env.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsEP4kkTOLqm",
        "outputId": "a3d250df-00ca-4783-f96b-92bdcbf3117b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = env.state.reshape(-1, 10)\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQPB8asti9xu",
        "outputId": "fd2854d5-efa8-45a7-d44d-16fc29657dc9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aafde9e-2419-40da-8ed6-d19e3eda56a4",
        "id": "-0r3lysXOrjZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:437 Info:{'opp_score': 437}\n",
            "Episode:2 Score:618 Info:{'opp_score': 388}\n",
            "Episode:3 Score:271 Info:{'opp_score': 546}\n",
            "Episode:4 Score:806 Info:{'opp_score': 291}\n",
            "Episode:5 Score:121 Info:{'opp_score': 611}\n",
            "Episode:6 Score:106 Info:{'opp_score': 586}\n",
            "Episode:7 Score:92 Info:{'opp_score': 642}\n",
            "Episode:8 Score:630 Info:{'opp_score': 355}\n",
            "Episode:9 Score:149 Info:{'opp_score': 574}\n",
            "Episode:10 Score:285 Info:{'opp_score': 525}\n",
            "Episode:11 Score:120 Info:{'opp_score': 535}\n",
            "Episode:12 Score:420 Info:{'opp_score': 450}\n",
            "Episode:13 Score:606 Info:{'opp_score': 406}\n",
            "Episode:14 Score:146 Info:{'opp_score': 591}\n",
            "Episode:15 Score:646 Info:{'opp_score': 371}\n",
            "Episode:16 Score:397 Info:{'opp_score': 497}\n",
            "Episode:17 Score:446 Info:{'opp_score': 446}\n",
            "Episode:18 Score:437 Info:{'opp_score': 462}\n",
            "Episode:19 Score:441 Info:{'opp_score': 441}\n",
            "Episode:20 Score:445 Info:{'opp_score': 440}\n",
            "Episode:21 Score:99 Info:{'opp_score': 619}\n",
            "Episode:22 Score:417 Info:{'opp_score': 412}\n",
            "Episode:23 Score:450 Info:{'opp_score': 445}\n",
            "Episode:24 Score:105 Info:{'opp_score': 595}\n",
            "Episode:25 Score:621 Info:{'opp_score': 376}\n",
            "Episode:26 Score:112 Info:{'opp_score': 567}\n",
            "Episode:27 Score:107 Info:{'opp_score': 587}\n",
            "Episode:28 Score:261 Info:{'opp_score': 551}\n",
            "Episode:29 Score:401 Info:{'opp_score': 476}\n",
            "Episode:30 Score:788 Info:{'opp_score': 318}\n",
            "Episode:31 Score:438 Info:{'opp_score': 433}\n",
            "Episode:32 Score:626 Info:{'opp_score': 366}\n",
            "Episode:33 Score:814 Info:{'opp_score': 279}\n",
            "Episode:34 Score:624 Info:{'opp_score': 384}\n",
            "Episode:35 Score:476 Info:{'opp_score': 416}\n",
            "Episode:36 Score:637 Info:{'opp_score': 357}\n",
            "Episode:37 Score:455 Info:{'opp_score': 450}\n",
            "Episode:38 Score:513 Info:{'opp_score': 388}\n",
            "Episode:39 Score:411 Info:{'opp_score': 471}\n",
            "Episode:40 Score:97 Info:{'opp_score': 627}\n",
            "Episode:41 Score:271 Info:{'opp_score': 531}\n",
            "Episode:42 Score:818 Info:{'opp_score': 273}\n",
            "Episode:43 Score:102 Info:{'opp_score': 607}\n",
            "Episode:44 Score:412 Info:{'opp_score': 477}\n",
            "Episode:45 Score:141 Info:{'opp_score': 581}\n",
            "Episode:46 Score:475 Info:{'opp_score': 435}\n",
            "Episode:47 Score:794 Info:{'opp_score': 309}\n",
            "Episode:48 Score:411 Info:{'opp_score': 466}\n",
            "Episode:49 Score:101 Info:{'opp_score': 606}\n",
            "Episode:50 Score:156 Info:{'opp_score': 596}\n",
            "Episode:51 Score:114 Info:{'opp_score': 554}\n",
            "Episode:52 Score:455 Info:{'opp_score': 450}\n",
            "Episode:53 Score:594 Info:{'opp_score': 344}\n",
            "Episode:54 Score:617 Info:{'opp_score': 387}\n",
            "Episode:55 Score:453 Info:{'opp_score': 448}\n",
            "Episode:56 Score:406 Info:{'opp_score': 506}\n",
            "Episode:57 Score:423 Info:{'opp_score': 473}\n",
            "Episode:58 Score:790 Info:{'opp_score': 315}\n",
            "Episode:59 Score:460 Info:{'opp_score': 460}\n",
            "Episode:60 Score:439 Info:{'opp_score': 439}\n",
            "Episode:61 Score:615 Info:{'opp_score': 395}\n",
            "Episode:62 Score:153 Info:{'opp_score': 568}\n",
            "Episode:63 Score:383 Info:{'opp_score': 458}\n",
            "Episode:64 Score:824 Info:{'opp_score': 264}\n",
            "Episode:65 Score:456 Info:{'opp_score': 451}\n",
            "Episode:66 Score:108 Info:{'opp_score': 583}\n",
            "Episode:67 Score:136 Info:{'opp_score': 581}\n",
            "Episode:68 Score:431 Info:{'opp_score': 456}\n",
            "Episode:69 Score:424 Info:{'opp_score': 474}\n",
            "Episode:70 Score:428 Info:{'opp_score': 463}\n",
            "Episode:71 Score:103 Info:{'opp_score': 603}\n",
            "Episode:72 Score:442 Info:{'opp_score': 447}\n",
            "Episode:73 Score:101 Info:{'opp_score': 606}\n",
            "Episode:74 Score:108 Info:{'opp_score': 583}\n",
            "Episode:75 Score:479 Info:{'opp_score': 454}\n",
            "Episode:76 Score:621 Info:{'opp_score': 366}\n",
            "Episode:77 Score:469 Info:{'opp_score': 464}\n",
            "Episode:78 Score:448 Info:{'opp_score': 443}\n",
            "Episode:79 Score:159 Info:{'opp_score': 604}\n",
            "Episode:80 Score:447 Info:{'opp_score': 442}\n",
            "Episode:81 Score:299 Info:{'opp_score': 539}\n",
            "Episode:82 Score:458 Info:{'opp_score': 453}\n",
            "Episode:83 Score:99 Info:{'opp_score': 629}\n",
            "Episode:84 Score:812 Info:{'opp_score': 282}\n",
            "Episode:85 Score:624 Info:{'opp_score': 374}\n",
            "Episode:86 Score:275 Info:{'opp_score': 510}\n",
            "Episode:87 Score:411 Info:{'opp_score': 476}\n",
            "Episode:88 Score:109 Info:{'opp_score': 579}\n",
            "Episode:89 Score:469 Info:{'opp_score': 469}\n",
            "Episode:90 Score:441 Info:{'opp_score': 481}\n",
            "Episode:91 Score:599 Info:{'opp_score': 364}\n",
            "Episode:92 Score:449 Info:{'opp_score': 444}\n",
            "Episode:93 Score:176 Info:{'opp_score': 621}\n",
            "Episode:94 Score:101 Info:{'opp_score': 611}\n",
            "Episode:95 Score:633 Info:{'opp_score': 398}\n",
            "Episode:96 Score:454 Info:{'opp_score': 449}\n",
            "Episode:97 Score:487 Info:{'opp_score': 402}\n",
            "Episode:98 Score:234 Info:{'opp_score': 484}\n",
            "Episode:99 Score:439 Info:{'opp_score': 439}\n",
            "Episode:100 Score:461 Info:{'opp_score': 461}\n",
            "Win Rate: 0.42\n"
          ]
        }
      ],
      "source": [
        "env = PrisonEnv()\n",
        "episodes = 100\n",
        "done = False\n",
        "score = 0\n",
        "wins=0\n",
        "for episode in range(1,episodes+1):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  score=0\n",
        "  while not done:\n",
        "      action = env.action_space.sample()\n",
        "      n_state, reward, done, info = env.step(action)\n",
        "      score+=reward\n",
        "  if info['opp_score'] < score:\n",
        "    wins += 1\n",
        "\n",
        "  print('Episode:{} Score:{} Info:{}'.format(episode, score, info))\n",
        "\n",
        "print('Win Rate:',wins/100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJIFZb40O-E8",
        "outputId": "c0ba643a-a02a-4ef6-f173-b186b3a1039f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KjcbSkClOrjo"
      },
      "outputs": [],
      "source": [
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbb9c16-2b8e-415a-b66c-3966d0600c60",
        "id": "9ZNE5MNMOrjo"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b27be5-601e-428c-c8cb-c498ff96c88a",
        "id": "yhPOduErOrjp"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Z9WusH-TOrjp"
      },
      "outputs": [],
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(1,states)))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(actions, activation='softmax'))\n",
        "    model.add(Flatten())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "njOepqZQOrjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "289f9378-99a1-4b7a-9379-613b4a92293e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a7ab210f85a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5yl-npaCOrjp"
      },
      "outputs": [],
      "source": [
        "model = build_model(states, actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cffcc7-538c-4295-b68b-e56e093a0c34",
        "id": "TnjmS3j4Orjp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1, 128)            1408      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1, 128)            16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1, 128)            16512     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1, 2)              258       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34690 (135.51 KB)\n",
            "Trainable params: 34690 (135.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82a1fa9-83ea-4608-d47a-099ae01806af",
        "id": "EvI5ZhxmOrjp"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 1, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UpZgtVWwTc4",
        "outputId": "44b629a1-e732-4aac-d3f3-de63d250bfe0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fagM9-iwgMcF",
        "outputId": "07464a46-6adc-4429-bd4f-7c64ea4dab3f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "M2mljTOnOrjq"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = BoltzmannQPolicy()\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279938eb-4524-4821-b636-aaa5d24f31cf",
        "id": "s4TrL0bMOrjq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "\r    1/10000 [..............................] - ETA: 15:53 - reward: 3.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   30/10000 [..............................] - ETA: 4:57 - reward: 0.8333"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 90s 9ms/step - reward: 2.0185\n",
            "50 episodes - episode_reward: 403.700 [141.000, 908.000] - loss: 3.376 - mae: 1.132 - mean_q: 0.999 - opp_score: 165.286\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 88s 9ms/step - reward: 1.8739\n",
            "50 episodes - episode_reward: 374.780 [143.000, 898.000] - loss: 4.152 - mae: 1.266 - mean_q: 1.000 - opp_score: 169.387\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 90s 9ms/step - reward: 1.6375\n",
            "50 episodes - episode_reward: 327.500 [142.000, 898.000] - loss: 3.708 - mae: 1.187 - mean_q: 1.000 - opp_score: 178.236\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 1.9818\n",
            "50 episodes - episode_reward: 396.360 [138.000, 902.000] - loss: 3.748 - mae: 1.196 - mean_q: 1.000 - opp_score: 166.320\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 2.1338\n",
            "done, took 454.939 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ad5eb511690>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False,verbose=0)"
      ],
      "metadata": {
        "id": "_ikpNdeY8vYo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "wins=0\n",
        "\n",
        "for episode in range(100):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = dqn.forward(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "          if total_reward > info['opp_score']:\n",
        "              wins +=1\n",
        "\n",
        "\n",
        "print(f'Win Rate: {wins/100}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0aBBpPuy1a0",
        "outputId": "f352152b-6879-4a60-b1cc-7a7ef181fcae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Win Rate: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size=int(1e5)):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "PUPzGnO4RyxC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = ReplayBuffer()\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mae', optimizer=tf.keras.optimizers.legacy.Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        minibatch = self.memory.sample(batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "Ec1FwpBNfm8n"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = PrisonEnv()\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = MyDQNAgent(state_size, action_size)\n",
        "episodes = 20\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    episode_reward=0\n",
        "    for time in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += episode_reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.memory.store(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(f\"Episode: {e+1}/{episodes}, Score: {episode_reward}, Epsilon: {agent.epsilon:.2}\")\n",
        "            break\n",
        "        if len(agent.memory) > 32:\n",
        "            agent.learn(32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTk7Zp7wfqaO",
        "outputId": "59e74e04-cf86-4aa8-e565-0514102163a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1/20, Score: 0, Epsilon: 0.43\n",
            "Episode: 2/20, Score: 0, Epsilon: 0.16\n",
            "Episode: 3/20, Score: 0, Epsilon: 0.059\n",
            "Episode: 4/20, Score: 0, Epsilon: 0.022\n",
            "Episode: 5/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 6/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 7/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 8/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 9/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 10/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 11/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 12/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 13/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 14/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 15/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 16/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 17/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 18/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 19/20, Score: 0, Epsilon: 0.01\n",
            "Episode: 20/20, Score: 0, Epsilon: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.epsilon = 0\n",
        "test_episodes = 100\n",
        "wins=0\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(500):\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "          if episode_reward > info['opp_score']:\n",
        "            wins += 1\n",
        "            break\n",
        "\n",
        "    print(f\"Test Episode {episode+1}/{test_episodes} Reward: {episode_reward} Opponent Score: {info['opp_score']}\")\n",
        "\n",
        "print(f\"Win Rate: {wins/100}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0lO3ZgIgOLl",
        "outputId": "54bc9df2-a015-489d-b750-19be474748f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Episode 1/100 Reward: 592 Opponent Score: 102\n",
            "Test Episode 2/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 3/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 4/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 5/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 6/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 7/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 8/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 9/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 10/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 11/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 12/100 Reward: 616 Opponent Score: 96\n",
            "Test Episode 13/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 14/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 15/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 16/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 17/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 18/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 19/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 20/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 21/100 Reward: 596 Opponent Score: 101\n",
            "Test Episode 22/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 23/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 24/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 25/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 26/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 27/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 28/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 29/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 30/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 31/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 32/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 33/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 34/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 35/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 36/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 37/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 38/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 39/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 40/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 41/100 Reward: 588 Opponent Score: 103\n",
            "Test Episode 42/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 43/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 44/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 45/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 46/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 47/100 Reward: 584 Opponent Score: 104\n",
            "Test Episode 48/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 49/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 50/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 51/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 52/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 53/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 54/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 55/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 56/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 57/100 Reward: 676 Opponent Score: 81\n",
            "Test Episode 58/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 59/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 60/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 61/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 62/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 63/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 64/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 65/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 66/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 67/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 68/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 69/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 70/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 71/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 72/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 73/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 74/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 75/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 76/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 77/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 78/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 79/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 80/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 81/100 Reward: 592 Opponent Score: 102\n",
            "Test Episode 82/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 83/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 84/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 85/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 86/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 87/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 88/100 Reward: 596 Opponent Score: 101\n",
            "Test Episode 89/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 90/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 91/100 Reward: 592 Opponent Score: 102\n",
            "Test Episode 92/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 93/100 Reward: 208 Opponent Score: 198\n",
            "Test Episode 94/100 Reward: 800 Opponent Score: 50\n",
            "Test Episode 95/100 Reward: 1000 Opponent Score: 0\n",
            "Test Episode 96/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 97/100 Reward: 500 Opponent Score: 500\n",
            "Test Episode 98/100 Reward: 552 Opponent Score: 112\n",
            "Test Episode 99/100 Reward: 204 Opponent Score: 199\n",
            "Test Episode 100/100 Reward: 532 Opponent Score: 117\n",
            "Win Rate: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Three Players\n"
      ],
      "metadata": {
        "id": "F91RSwv2ni_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tit_for_tat_all(round,opp1_move,opp2_move):\n",
        "  if round == 200:\n",
        "    return 1\n",
        "  else:\n",
        "    return opp1_move and opp2_move\n",
        "\n",
        "def tit_for_tat_any(round,opp1_move,opp2_move):\n",
        "  if round == 200:\n",
        "    return 1\n",
        "  else:\n",
        "    return opp1_move or opp2_move"
      ],
      "metadata": {
        "id": "T1koShSEP8tI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cooperate():\n",
        "  return 1\n",
        "\n",
        "def defect():\n",
        "  return 0\n",
        "\n",
        "def rand():\n",
        "  return random.randint(0,1)"
      ],
      "metadata": {
        "id": "AhVo-NXZP8tc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tit_for_two_tat_any(round, opp1_move1, opp1_move2, opp2_move1, opp2_move2):\n",
        "  if round >= 199 or opp1_move1 == 1 or opp1_move1 == 1 or opp2_move1 or opp2_move2:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def tit_for_two_tat_all(round, opp1_move1, opp1_move2, opp2_move1, opp2_move2):\n",
        "  if round >= 199 or (opp1_move1 == 1 or opp1_move2 == 1) and (opp2_move1 or opp2_move2):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "xpda_eu_P8td"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_grudger_any(last_defect1, last_defect2):\n",
        "  if last_defect1 < 4 or last_defect2 < 4:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def soft_grudger_all(last_defect1, last_defect2):\n",
        "  if last_defect1 < 4 and last_defect2 < 4:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "lmGsIhhVP8te"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreePlayers():\n",
        "    def __init__(self):\n",
        "        self.action_space = Discrete(2)                      # 0:defect 1:cooperate\n",
        "        self.observation_space = np.array([MultiBinary(10), MultiBinary(10)])\n",
        "        self.round = 200\n",
        "\n",
        "        self.state = np.array([[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]])\n",
        "\n",
        "        self.opp1_strategy = random.randint(0,9)\n",
        "        self.opp1_move1=self.state[0][0]\n",
        "        self.opp1_move2=self.state[0][1]\n",
        "\n",
        "        self.opp2_strategy = random.randint(0,9)\n",
        "        self.opp2_move1=self.state[1][0]\n",
        "        self.opp2_move2=self.state[1][1]\n",
        "\n",
        "        self.last_move1=1\n",
        "        self.last_move2=1\n",
        "\n",
        "        self.opp1_defected=False\n",
        "        self.opp2_defected=False\n",
        "        self.user_defected=False\n",
        "\n",
        "        self.last_defect= np.Infinity\n",
        "        self.opp1_last_defect= np.Infinity\n",
        "        self.opp2_last_defect= np.Infinity\n",
        "        self.move=np.array([random.randint(0,1),random.randint(0,1)])\n",
        "\n",
        "        self.opp1_score = 0\n",
        "        self.opp2_score = 0\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self.opp1_move1=self.state[0][0]\n",
        "        self.opp1_move2=self.state[0][1]\n",
        "\n",
        "        self.opp2_move1=self.state[1][0]\n",
        "        self.opp2_move2=self.state[1][1]\n",
        "\n",
        "        self.last_move2=self.last_move1\n",
        "        self.last_move1=self.move\n",
        "\n",
        "        self.move=action\n",
        "\n",
        "\n",
        "        if self.opp1_strategy == 0:\n",
        "          opp1_move=tit_for_tat_any(self.round,self.last_move1, self.opp2_move1)\n",
        "        elif self.opp1_strategy == 1:\n",
        "          opp1_move=tit_for_tat_all(self.round,self.last_move1, self.opp2_move1)\n",
        "        elif self.opp1_strategy == 2:\n",
        "          opp1_move=tit_for_two_tat_all(self.round, self.last_move1, self.last_move2, self.opp2_move1, self.opp2_move2)\n",
        "        elif self.opp1_strategy == 3:\n",
        "          opp1_move=tit_for_two_tat_any(self.round, self.last_move1, self.last_move2, self.opp2_move1, self.opp2_move2)\n",
        "        elif self.opp1_strategy == 4:\n",
        "          opp1_move=soft_grudger_any(self.last_defect, self.opp2_last_defect)\n",
        "        elif self.opp1_strategy == 5:\n",
        "          opp1_move=soft_grudger_all(self.last_defect, self.opp2_last_defect)\n",
        "\n",
        "        if self.opp2_strategy == 0:\n",
        "          opp2_move=tit_for_tat_any(self.round,self.last_move1, self.opp1_move1)\n",
        "        elif self.opp2_strategy == 1:\n",
        "          opp2_move=tit_for_tat_all(self.round,self.last_move1, self.opp1_move1)\n",
        "        elif self.opp2_strategy == 2:\n",
        "          opp2_move=tit_for_two_tat_all(self.round, self.last_move1, self.last_move2, self.opp1_move1, self.opp1_move2)\n",
        "        elif self.opp2_strategy == 3:\n",
        "          opp2_move=tit_for_two_tat_any(self.round, self.last_move1, self.last_move2, self.opp1_move1, self.opp1_move2)\n",
        "        elif self.opp2_strategy == 4:\n",
        "          opp2_move=soft_grudger_any(self.last_defect, self.opp1_last_defect)\n",
        "        elif self.opp2_strategy == 5:\n",
        "          opp2_move=soft_grudger_all(self.last_defect, self.opp1_last_defect)\n",
        "\n",
        "\n",
        "\n",
        "        temp = np.array(list(self.state[0]))\n",
        "        for i in range(1,10):\n",
        "          self.state[0][i] = temp[i-1]\n",
        "        self.state[0][0] = opp1_move\n",
        "\n",
        "        temp = np.array(list(self.state[1]))\n",
        "        for i in range(1,10):\n",
        "          self.state[1][i] = temp[i-1]\n",
        "        self.state[1][0] = opp2_move\n",
        "\n",
        "\n",
        "\n",
        "        if opp1_move == 0:\n",
        "          self.opp1_defected=True\n",
        "          self.opp1_last_defect=0\n",
        "        else:\n",
        "          self.opp1_last_defect += 1\n",
        "\n",
        "        if opp2_move == 0:\n",
        "          self.opp2_defected=True\n",
        "          self.opp2_last_defect=0\n",
        "        else:\n",
        "          self.opp2_last_defect += 1\n",
        "\n",
        "        if self.move == 0:\n",
        "          self.user_defected=True\n",
        "          self.last_defect=0\n",
        "        else:\n",
        "          self.last_defect += 1\n",
        "\n",
        "\n",
        "\n",
        "        if self.move == 1 and opp1_move == 1 and opp2_move ==  1:\n",
        "          reward = 4\n",
        "          self.opp1_score += 4\n",
        "          self.opp2_score += 4\n",
        "        elif self.move == 1 and opp1_move == 1 and opp2_move == 0:\n",
        "          reward = 0\n",
        "          self.opp1_score += 0\n",
        "          self.opp2_score += 10\n",
        "        elif self.move == 1 and opp1_move == 0 and opp2_move == 1:\n",
        "          reward = 0\n",
        "          self.opp1_score += 10\n",
        "          self.opp2_score += 0\n",
        "        elif self.move == 1 and opp1_move == 0 and opp2_move == 0:\n",
        "          reward = 0\n",
        "          self.opp1_score += 5\n",
        "          self.opp2_score += 5\n",
        "        elif self.move == 0 and opp1_move == 1 and opp2_move ==  1:\n",
        "          reward = 10\n",
        "          self.opp1_score += 0\n",
        "          self.opp2_score += 0\n",
        "        elif self.move == 0 and opp1_move == 1 and opp2_move == 0:\n",
        "          reward = 5\n",
        "          self.opp1_score += 0\n",
        "          self.opp2_score += 5\n",
        "        elif self.move == 0 and opp1_move == 0 and opp2_move == 1:\n",
        "          reward = 5\n",
        "          self.opp1_score += 5\n",
        "          self.opp2_score += 0\n",
        "        elif self.move == 0 and opp1_move == 0 and opp2_move == 0:\n",
        "          reward = 2\n",
        "          self.opp1_score += 2\n",
        "          self.opp2_score += 2\n",
        "\n",
        "        self.round -= 1\n",
        "\n",
        "        if self.round == 0:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        info = {'opp1_score':self.opp1_score, 'opp2_score':self.opp2_score}\n",
        "\n",
        "\n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self,mode):\n",
        "        # Implement viz\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.round = 200\n",
        "        self.opp1_strategy = random.randint(0,5)\n",
        "        self.opp2_strategy = random.randint(0,5)\n",
        "        self.state = np.array([[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]])\n",
        "        self.opp1_move1=self.state[0][0]\n",
        "        self.opp1_move2=self.state[0][1]\n",
        "        self.opp2_move1=self.state[1][0]\n",
        "        self.opp2_move2=self.state[1][1]\n",
        "        self.last_move1=1\n",
        "        self.last_move2=1\n",
        "        self.opp1_defected=False\n",
        "        self.opp2_defected=False\n",
        "        self.user_defected=False\n",
        "        self.last_defect= np.Infinity\n",
        "        self.opp1_last_defect= np.Infinity\n",
        "        self.opp2_last_defect= np.Infinity\n",
        "        self.gamma=0.9\n",
        "        self.move=random.randint(0,1)\n",
        "        self.reward = 0\n",
        "\n",
        "        self.opp1_score = 0\n",
        "        self.opp2_score = 0\n",
        "\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "lNHr3nVZIbmz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = ThreePlayers()"
      ],
      "metadata": {
        "id": "egnnZkoDIbm1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88490c53-50f7-4a36-f946-f82792b6ccdc",
        "id": "5ohQCg-nIbm2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d059cb-5dfe-4e69-ac7f-ccb5205c38f4",
        "id": "4naBMuZUIbm3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 2\n",
            "Episode:1 Score:246 Info:{'opp1_score': 686, 'opp2_score': 686}\n",
            "0 3\n",
            "Episode:2 Score:1388 Info:{'opp1_score': 408, 'opp2_score': 408}\n",
            "3 3\n",
            "Episode:3 Score:1418 Info:{'opp1_score': 388, 'opp2_score': 388}\n",
            "0 3\n",
            "Episode:4 Score:1340 Info:{'opp1_score': 440, 'opp2_score': 440}\n",
            "4 2\n",
            "Episode:5 Score:208 Info:{'opp1_score': 708, 'opp2_score': 688}\n",
            "2 1\n",
            "Episode:6 Score:285 Info:{'opp1_score': 655, 'opp2_score': 690}\n",
            "3 1\n",
            "Episode:7 Score:815 Info:{'opp1_score': 305, 'opp2_score': 900}\n",
            "0 4\n",
            "Episode:8 Score:402 Info:{'opp1_score': 352, 'opp2_score': 1112}\n",
            "2 0\n",
            "Episode:9 Score:1090 Info:{'opp1_score': 635, 'opp2_score': 445}\n",
            "0 1\n",
            "Episode:10 Score:700 Info:{'opp1_score': 415, 'opp2_score': 865}\n",
            "5 5\n",
            "Episode:11 Score:1424 Info:{'opp1_score': 384, 'opp2_score': 384}\n",
            "4 0\n",
            "Episode:12 Score:385 Info:{'opp1_score': 1030, 'opp2_score': 365}\n",
            "4 2\n",
            "Episode:13 Score:231 Info:{'opp1_score': 671, 'opp2_score': 666}\n",
            "4 1\n",
            "Episode:14 Score:226 Info:{'opp1_score': 681, 'opp2_score': 681}\n",
            "5 4\n",
            "Episode:15 Score:237 Info:{'opp1_score': 652, 'opp2_score': 737}\n",
            "1 3\n",
            "Episode:16 Score:809 Info:{'opp1_score': 894, 'opp2_score': 279}\n",
            "3 3\n",
            "Episode:17 Score:1442 Info:{'opp1_score': 372, 'opp2_score': 372}\n",
            "4 4\n",
            "Episode:18 Score:192 Info:{'opp1_score': 722, 'opp2_score': 722}\n",
            "2 3\n",
            "Episode:19 Score:1161 Info:{'opp1_score': 626, 'opp2_score': 391}\n",
            "5 4\n",
            "Episode:20 Score:233 Info:{'opp1_score': 648, 'opp2_score': 773}\n",
            "5 4\n",
            "Episode:21 Score:239 Info:{'opp1_score': 634, 'opp2_score': 799}\n",
            "2 0\n",
            "Episode:22 Score:1139 Info:{'opp1_score': 609, 'opp2_score': 364}\n",
            "1 0\n",
            "Episode:23 Score:707 Info:{'opp1_score': 847, 'opp2_score': 392}\n",
            "0 5\n",
            "Episode:24 Score:1352 Info:{'opp1_score': 432, 'opp2_score': 432}\n",
            "5 2\n",
            "Episode:25 Score:277 Info:{'opp1_score': 677, 'opp2_score': 672}\n",
            "0 0\n",
            "Episode:26 Score:1376 Info:{'opp1_score': 416, 'opp2_score': 416}\n",
            "0 0\n",
            "Episode:27 Score:1376 Info:{'opp1_score': 416, 'opp2_score': 416}\n",
            "5 5\n",
            "Episode:28 Score:1418 Info:{'opp1_score': 388, 'opp2_score': 388}\n",
            "4 4\n",
            "Episode:29 Score:216 Info:{'opp1_score': 686, 'opp2_score': 686}\n",
            "2 3\n",
            "Episode:30 Score:1139 Info:{'opp1_score': 624, 'opp2_score': 379}\n",
            "5 1\n",
            "Episode:31 Score:246 Info:{'opp1_score': 686, 'opp2_score': 746}\n",
            "4 4\n",
            "Episode:32 Score:222 Info:{'opp1_score': 692, 'opp2_score': 692}\n",
            "2 4\n",
            "Episode:33 Score:216 Info:{'opp1_score': 686, 'opp2_score': 706}\n",
            "1 3\n",
            "Episode:34 Score:839 Info:{'opp1_score': 934, 'opp2_score': 229}\n",
            "2 1\n",
            "Episode:35 Score:231 Info:{'opp1_score': 666, 'opp2_score': 671}\n",
            "1 1\n",
            "Episode:36 Score:204 Info:{'opp1_score': 724, 'opp2_score': 724}\n",
            "5 2\n",
            "Episode:37 Score:361 Info:{'opp1_score': 686, 'opp2_score': 671}\n",
            "1 2\n",
            "Episode:38 Score:273 Info:{'opp1_score': 763, 'opp2_score': 688}\n",
            "0 4\n",
            "Episode:39 Score:392 Info:{'opp1_score': 362, 'opp2_score': 1002}\n",
            "2 1\n",
            "Episode:40 Score:233 Info:{'opp1_score': 693, 'opp2_score': 708}\n",
            "5 4\n",
            "Episode:41 Score:231 Info:{'opp1_score': 656, 'opp2_score': 741}\n",
            "0 1\n",
            "Episode:42 Score:699 Info:{'opp1_score': 384, 'opp2_score': 859}\n",
            "0 5\n",
            "Episode:43 Score:1454 Info:{'opp1_score': 364, 'opp2_score': 364}\n",
            "0 5\n",
            "Episode:44 Score:1346 Info:{'opp1_score': 436, 'opp2_score': 436}\n",
            "3 3\n",
            "Episode:45 Score:1418 Info:{'opp1_score': 388, 'opp2_score': 388}\n",
            "5 4\n",
            "Episode:46 Score:249 Info:{'opp1_score': 634, 'opp2_score': 649}\n",
            "4 5\n",
            "Episode:47 Score:239 Info:{'opp1_score': 714, 'opp2_score': 649}\n",
            "0 3\n",
            "Episode:48 Score:1400 Info:{'opp1_score': 400, 'opp2_score': 400}\n",
            "1 2\n",
            "Episode:49 Score:193 Info:{'opp1_score': 733, 'opp2_score': 728}\n",
            "2 3\n",
            "Episode:50 Score:1119 Info:{'opp1_score': 649, 'opp2_score': 364}\n",
            "1 1\n",
            "Episode:51 Score:224 Info:{'opp1_score': 674, 'opp2_score': 674}\n",
            "4 1\n",
            "Episode:52 Score:172 Info:{'opp1_score': 752, 'opp2_score': 752}\n",
            "5 2\n",
            "Episode:53 Score:263 Info:{'opp1_score': 748, 'opp2_score': 683}\n",
            "4 2\n",
            "Episode:54 Score:219 Info:{'opp1_score': 689, 'opp2_score': 684}\n",
            "1 2\n",
            "Episode:55 Score:239 Info:{'opp1_score': 724, 'opp2_score': 699}\n",
            "0 3\n",
            "Episode:56 Score:1406 Info:{'opp1_score': 396, 'opp2_score': 396}\n",
            "2 5\n",
            "Episode:57 Score:367 Info:{'opp1_score': 717, 'opp2_score': 682}\n",
            "0 0\n",
            "Episode:58 Score:1430 Info:{'opp1_score': 380, 'opp2_score': 380}\n",
            "4 2\n",
            "Episode:59 Score:195 Info:{'opp1_score': 725, 'opp2_score': 720}\n",
            "4 5\n",
            "Episode:60 Score:219 Info:{'opp1_score': 759, 'opp2_score': 664}\n",
            "4 5\n",
            "Episode:61 Score:251 Info:{'opp1_score': 706, 'opp2_score': 631}\n",
            "3 1\n",
            "Episode:62 Score:816 Info:{'opp1_score': 256, 'opp2_score': 916}\n",
            "2 3\n",
            "Episode:63 Score:1134 Info:{'opp1_score': 619, 'opp2_score': 369}\n",
            "2 4\n",
            "Episode:64 Score:219 Info:{'opp1_score': 694, 'opp2_score': 699}\n",
            "0 2\n",
            "Episode:65 Score:1102 Info:{'opp1_score': 352, 'opp2_score': 632}\n",
            "2 4\n",
            "Episode:66 Score:188 Info:{'opp1_score': 718, 'opp2_score': 738}\n",
            "0 4\n",
            "Episode:67 Score:415 Info:{'opp1_score': 355, 'opp2_score': 1120}\n",
            "1 2\n",
            "Episode:68 Score:271 Info:{'opp1_score': 666, 'opp2_score': 631}\n",
            "2 1\n",
            "Episode:69 Score:207 Info:{'opp1_score': 702, 'opp2_score': 707}\n",
            "2 5\n",
            "Episode:70 Score:293 Info:{'opp1_score': 638, 'opp2_score': 673}\n",
            "3 2\n",
            "Episode:71 Score:1140 Info:{'opp1_score': 355, 'opp2_score': 635}\n",
            "0 1\n",
            "Episode:72 Score:722 Info:{'opp1_score': 357, 'opp2_score': 857}\n",
            "1 1\n",
            "Episode:73 Score:210 Info:{'opp1_score': 705, 'opp2_score': 705}\n",
            "1 0\n",
            "Episode:74 Score:670 Info:{'opp1_score': 830, 'opp2_score': 360}\n",
            "1 3\n",
            "Episode:75 Score:796 Info:{'opp1_score': 881, 'opp2_score': 261}\n",
            "3 0\n",
            "Episode:76 Score:1424 Info:{'opp1_score': 384, 'opp2_score': 384}\n",
            "2 1\n",
            "Episode:77 Score:217 Info:{'opp1_score': 697, 'opp2_score': 712}\n",
            "0 4\n",
            "Episode:78 Score:389 Info:{'opp1_score': 349, 'opp2_score': 1054}\n",
            "4 2\n",
            "Episode:79 Score:238 Info:{'opp1_score': 683, 'opp2_score': 663}\n",
            "1 5\n",
            "Episode:80 Score:247 Info:{'opp1_score': 742, 'opp2_score': 677}\n",
            "2 1\n",
            "Episode:81 Score:227 Info:{'opp1_score': 682, 'opp2_score': 697}\n",
            "3 5\n",
            "Episode:82 Score:1418 Info:{'opp1_score': 388, 'opp2_score': 388}\n",
            "0 0\n",
            "Episode:83 Score:1328 Info:{'opp1_score': 448, 'opp2_score': 448}\n",
            "3 3\n",
            "Episode:84 Score:1514 Info:{'opp1_score': 324, 'opp2_score': 324}\n",
            "3 4\n",
            "Episode:85 Score:406 Info:{'opp1_score': 331, 'opp2_score': 1111}\n",
            "0 3\n",
            "Episode:86 Score:1370 Info:{'opp1_score': 420, 'opp2_score': 420}\n",
            "0 0\n",
            "Episode:87 Score:1418 Info:{'opp1_score': 388, 'opp2_score': 388}\n",
            "0 4\n",
            "Episode:88 Score:404 Info:{'opp1_score': 339, 'opp2_score': 1069}\n",
            "1 1\n",
            "Episode:89 Score:214 Info:{'opp1_score': 689, 'opp2_score': 689}\n",
            "5 3\n",
            "Episode:90 Score:1370 Info:{'opp1_score': 420, 'opp2_score': 420}\n",
            "1 3\n",
            "Episode:91 Score:809 Info:{'opp1_score': 904, 'opp2_score': 309}\n",
            "4 3\n",
            "Episode:92 Score:401 Info:{'opp1_score': 1131, 'opp2_score': 336}\n",
            "2 3\n",
            "Episode:93 Score:1121 Info:{'opp1_score': 621, 'opp2_score': 356}\n",
            "0 5\n",
            "Episode:94 Score:1430 Info:{'opp1_score': 380, 'opp2_score': 380}\n",
            "3 5\n",
            "Episode:95 Score:1406 Info:{'opp1_score': 396, 'opp2_score': 396}\n",
            "1 3\n",
            "Episode:96 Score:828 Info:{'opp1_score': 918, 'opp2_score': 258}\n",
            "3 2\n",
            "Episode:97 Score:1121 Info:{'opp1_score': 381, 'opp2_score': 636}\n",
            "4 2\n",
            "Episode:98 Score:205 Info:{'opp1_score': 715, 'opp2_score': 710}\n",
            "5 3\n",
            "Episode:99 Score:1394 Info:{'opp1_score': 404, 'opp2_score': 404}\n",
            "4 4\n",
            "Episode:100 Score:200 Info:{'opp1_score': 710, 'opp2_score': 710}\n",
            "0.35\n"
          ]
        }
      ],
      "source": [
        "env = ThreePlayers()\n",
        "episodes = 100\n",
        "wins=0\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    print(env.opp1_strategy, env.opp2_strategy)\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    if score > info['opp1_score'] and score > info['opp2_score']:\n",
        "      wins += 1\n",
        "\n",
        "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))\n",
        "print(wins/100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82e6d41-9238-4aa6-edcc-ce6dfe910027",
        "id": "ccnV6xHjIbm3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f485be5b-5455-4de5-bd9f-bff9b99a7ae0",
        "id": "voYc0Cl-Ibm4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "m0XLZS1UIbm4"
      },
      "outputs": [],
      "source": [
        "states = env.state.shape\n",
        "actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1522e5b-068a-4945-c4b6-2140e50c31ce",
        "id": "b26lnG8jIbm4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee8679b-39ca-48c0-8bb3-357806e538f0",
        "id": "u9DNIToGIbm5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xkgaXwAEIbm6"
      },
      "outputs": [],
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(1, 2, 10), input_dim=3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='softmax'))\n",
        "    model.add(Flatten())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vJqJWZRYIbm6"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ehapVp1HIbm6"
      },
      "outputs": [],
      "source": [
        "model = build_model(states, actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d773e1c-f0a6-44c6-fed5-2b25e0652fee",
        "id": "1Br86VVNIbm7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 1, 2, 128)         1408      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1, 2, 128)         16512     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1, 2, 128)         16512     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1, 2, 1)           129       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34561 (135.00 KB)\n",
            "Trainable params: 34561 (135.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b01ab16d-334c-4215-d886-7c3a973c9ed3",
        "id": "XG_1py7SIbm7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 1, 2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6b09f8-516d-4eb2-9027-28babf4244e1",
        "id": "kZytWtZgIbm7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854f8fff-5bf3-49f8-a845-66d2ba5fe5f0",
        "id": "zdWSVwiwIbm8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UgZWPCOjIbm8"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = BoltzmannQPolicy()\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  nb_actions=actions)\n",
        "    return dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93db0a7-9efc-48e2-f5d1-41761e51788c",
        "id": "ka_brS0pIbm8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 25000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 85s 8ms/step - reward: 3.2532\n",
            "50 episodes - episode_reward: 650.640 [188.000, 1442.000] - loss: 13.853 - mae: 2.403 - mean_q: 1.000 - opp1_score: 296.133 - opp2_score: 299.551\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 3.2598\n",
            "50 episodes - episode_reward: 651.960 [184.000, 1430.000] - loss: 11.192 - mae: 2.115 - mean_q: 1.000 - opp1_score: 306.261 - opp2_score: 295.124\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            " 4998/10000 [=============>................] - ETA: 47s - reward: 3.8201done, took 225.766 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ad5e7437eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),metrics=['mae'])\n",
        "#dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=25000, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False,verbose=1)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaWrwVb7QxnB",
        "outputId": "6fce2365-33fa-4375-ba84-93eebc4efecb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 414.000, steps: 200\n",
            "Episode 2: reward: 411.000, steps: 200\n",
            "Episode 3: reward: 411.000, steps: 200\n",
            "Episode 4: reward: 414.000, steps: 200\n",
            "Episode 5: reward: 411.000, steps: 200\n",
            "Episode 6: reward: 2000.000, steps: 200\n",
            "Episode 7: reward: 414.000, steps: 200\n",
            "Episode 8: reward: 411.000, steps: 200\n",
            "Episode 9: reward: 411.000, steps: 200\n",
            "Episode 10: reward: 411.000, steps: 200\n",
            "Episode 11: reward: 411.000, steps: 200\n",
            "Episode 12: reward: 2000.000, steps: 200\n",
            "Episode 13: reward: 416.000, steps: 200\n",
            "Episode 14: reward: 2000.000, steps: 200\n",
            "Episode 15: reward: 2000.000, steps: 200\n",
            "Episode 16: reward: 419.000, steps: 200\n",
            "Episode 17: reward: 408.000, steps: 200\n",
            "Episode 18: reward: 2000.000, steps: 200\n",
            "Episode 19: reward: 411.000, steps: 200\n",
            "Episode 20: reward: 411.000, steps: 200\n",
            "Episode 21: reward: 414.000, steps: 200\n",
            "Episode 22: reward: 2000.000, steps: 200\n",
            "Episode 23: reward: 419.000, steps: 200\n",
            "Episode 24: reward: 411.000, steps: 200\n",
            "Episode 25: reward: 411.000, steps: 200\n",
            "Episode 26: reward: 408.000, steps: 200\n",
            "Episode 27: reward: 408.000, steps: 200\n",
            "Episode 28: reward: 419.000, steps: 200\n",
            "Episode 29: reward: 2000.000, steps: 200\n",
            "Episode 30: reward: 411.000, steps: 200\n",
            "Episode 31: reward: 411.000, steps: 200\n",
            "Episode 32: reward: 2000.000, steps: 200\n",
            "Episode 33: reward: 411.000, steps: 200\n",
            "Episode 34: reward: 2000.000, steps: 200\n",
            "Episode 35: reward: 2000.000, steps: 200\n",
            "Episode 36: reward: 422.000, steps: 200\n",
            "Episode 37: reward: 2000.000, steps: 200\n",
            "Episode 38: reward: 419.000, steps: 200\n",
            "Episode 39: reward: 2000.000, steps: 200\n",
            "Episode 40: reward: 419.000, steps: 200\n",
            "Episode 41: reward: 408.000, steps: 200\n",
            "Episode 42: reward: 414.000, steps: 200\n",
            "Episode 43: reward: 416.000, steps: 200\n",
            "Episode 44: reward: 414.000, steps: 200\n",
            "Episode 45: reward: 422.000, steps: 200\n",
            "Episode 46: reward: 2000.000, steps: 200\n",
            "Episode 47: reward: 408.000, steps: 200\n",
            "Episode 48: reward: 408.000, steps: 200\n",
            "Episode 49: reward: 419.000, steps: 200\n",
            "Episode 50: reward: 2000.000, steps: 200\n",
            "Episode 51: reward: 2000.000, steps: 200\n",
            "Episode 52: reward: 419.000, steps: 200\n",
            "Episode 53: reward: 414.000, steps: 200\n",
            "Episode 54: reward: 408.000, steps: 200\n",
            "Episode 55: reward: 414.000, steps: 200\n",
            "Episode 56: reward: 414.000, steps: 200\n",
            "Episode 57: reward: 2000.000, steps: 200\n",
            "Episode 58: reward: 419.000, steps: 200\n",
            "Episode 59: reward: 419.000, steps: 200\n",
            "Episode 60: reward: 408.000, steps: 200\n",
            "Episode 61: reward: 422.000, steps: 200\n",
            "Episode 62: reward: 422.000, steps: 200\n",
            "Episode 63: reward: 2000.000, steps: 200\n",
            "Episode 64: reward: 414.000, steps: 200\n",
            "Episode 65: reward: 419.000, steps: 200\n",
            "Episode 66: reward: 422.000, steps: 200\n",
            "Episode 67: reward: 2000.000, steps: 200\n",
            "Episode 68: reward: 419.000, steps: 200\n",
            "Episode 69: reward: 411.000, steps: 200\n",
            "Episode 70: reward: 2000.000, steps: 200\n",
            "Episode 71: reward: 2000.000, steps: 200\n",
            "Episode 72: reward: 2000.000, steps: 200\n",
            "Episode 73: reward: 411.000, steps: 200\n",
            "Episode 74: reward: 408.000, steps: 200\n",
            "Episode 75: reward: 414.000, steps: 200\n",
            "Episode 76: reward: 411.000, steps: 200\n",
            "Episode 77: reward: 411.000, steps: 200\n",
            "Episode 78: reward: 2000.000, steps: 200\n",
            "Episode 79: reward: 408.000, steps: 200\n",
            "Episode 80: reward: 2000.000, steps: 200\n",
            "Episode 81: reward: 2000.000, steps: 200\n",
            "Episode 82: reward: 416.000, steps: 200\n",
            "Episode 83: reward: 411.000, steps: 200\n",
            "Episode 84: reward: 414.000, steps: 200\n",
            "Episode 85: reward: 411.000, steps: 200\n",
            "Episode 86: reward: 414.000, steps: 200\n",
            "Episode 87: reward: 411.000, steps: 200\n",
            "Episode 88: reward: 2000.000, steps: 200\n",
            "Episode 89: reward: 411.000, steps: 200\n",
            "Episode 90: reward: 408.000, steps: 200\n",
            "Episode 91: reward: 408.000, steps: 200\n",
            "Episode 92: reward: 408.000, steps: 200\n",
            "Episode 93: reward: 411.000, steps: 200\n",
            "Episode 94: reward: 411.000, steps: 200\n",
            "Episode 95: reward: 411.000, steps: 200\n",
            "Episode 96: reward: 2000.000, steps: 200\n",
            "Episode 97: reward: 2000.000, steps: 200\n",
            "Episode 98: reward: 411.000, steps: 200\n",
            "Episode 99: reward: 2000.000, steps: 200\n",
            "Episode 100: reward: 408.000, steps: 200\n",
            "857.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "wins=0\n",
        "\n",
        "for episode in range(100):  # Run 100 episodes\n",
        "    state = env.reset()  # Reset environment for new episode\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = dqn.forward(state)  # Get action from DQN agent\n",
        "        next_state, reward, done, info = env.step(action)  # Apply action to environment\n",
        "        total_reward += reward  # Accumulate reward\n",
        "        state = next_state  # Move to next state\n",
        "\n",
        "        if done:\n",
        "          if total_reward > info['opp1_score'] and total_reward > info['opp2_score']:\n",
        "              wins +=1\n",
        "\n",
        "\n",
        "print(f'Win Rate: {wins/100}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMh6n3Ln3nNN",
        "outputId": "861e9d74-6f93-4376-cc4d-daa8d3711e3e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Win Rate: 1.0\n"
          ]
        }
      ]
    }
  ]
}